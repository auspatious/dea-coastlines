{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vietnam Coastlines Combined\n",
    "\n",
    "\n",
    "* Load stack of all available Landsat 5, 7, 8 and 9 satellite imagery for a location \n",
    "* Convert each satellite image into a remote sensing water index (MNDWI)\n",
    "* For each satellite image, model ocean tides into a grid based on exact time of image acquisition\n",
    "* Interpolate tide heights into spatial extent of image stack using the [FES2014 global tide model](https://github.com/GeoscienceAustralia/dea-coastlines/wiki/Setting-up-tidal-models-for-DEA-Coastlines)\n",
    "* Mask out high and low tide pixels by removing all observations acquired outside of 50 percent of the observed tidal range centered over mean sea level\n",
    "* Combine tidally-masked data into annual median composites representing the most representative position of the coastline at approximately mean sea level each year\n",
    "* Apply morphological extraction algorithms to mask annual median composite rasters to a valid coastal region\n",
    "* Extract waterline vectors using subpixel waterline extraction ([Bishop-Taylor et al. 2019b](https://doi.org/10.3390/rs11242984))\n",
    "* Compute rates of coastal change at every 30 m using linear regression\n",
    "\n",
    "This is an interactive version of the code intended for prototyping; to run this analysis at scale, use the [command line tools](DEACoastlines_generation_CLI.ipynb).\n",
    "\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting started\n",
    "\n",
    "Set working directory to top level of repository to ensure links work correctly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd .."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load packages\n",
    "\n",
    "First we import the required Python packages, then we connect to the database, and load the catalog of virtual products."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install -r requirements.in --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ['USE_PYGEOS'] = '0'\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "# Load DEA Coastlines and DEA tools code\n",
    "import coastlines.raster\n",
    "import coastlines.utils\n",
    "import coastlines.vector\n",
    "\n",
    "from coastlines.utils import get_study_site_geometry\n",
    "from coastlines.combined import (\n",
    "    opinionated_stac_search,\n",
    "    load_and_mask_data_with_stac,\n",
    "    generate_yearly_combined_ds\n",
    ")\n",
    "\n",
    "# Load other libraries\n",
    "import folium\n",
    "import geohash as gh\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "from datacube.utils.dask import start_local_dask\n",
    "from datacube.utils.geometry import Geometry\n",
    "from dea_tools.coastal import pixel_tides\n",
    "from dea_tools.spatial import subpixel_contours\n",
    "from odc.algo import mask_cleanup, erase_bad, to_f32\n",
    "from odc.stac import configure_s3_access, load\n",
    "from pystac_client import Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create local dask client for parallelisation\n",
    "dask_client = start_local_dask(n_workers=12, mem_safety_margin=\"3GB\")\n",
    "\n",
    "# Configure S3 access including request payer\n",
    "_ = configure_s3_access(requester_pays=True, cloud_defaults=True)\n",
    "\n",
    "print(dask_client.dashboard_link)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Setup\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set analysis parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Study area selection\n",
    "# study_area = \"13,45\" # South west\n",
    "study_area = \"9,19\" # North\n",
    "# study_area = \"18,32\" # Central \n",
    "\n",
    "# Raster config\n",
    "raster_version = 'testing'\n",
    "start_year = 2012\n",
    "end_year = 2022\n",
    "baseline_year = 2021\n",
    "\n",
    "# Vector config\n",
    "vector_version = raster_version\n",
    "water_index = \"mndwi\"\n",
    "index_threshold = 0.0\n",
    "\n",
    "config_path = 'configs/vietnam_coastlines_config_development.yaml'\n",
    "\n",
    "# Output config\n",
    "output_dir = f\"data/interim/vector/{vector_version}/{study_area}_{vector_version}\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Load analysis params from config file\n",
    "config = coastlines.utils.load_config(\n",
    "    config_path=config_path)\n",
    "\n",
    "log = coastlines.utils.get_logger()\n",
    "\n",
    "# Load the geometry from the grid used for the location\n",
    "geometry, bbox = get_study_site_geometry(config[\"Input files\"][\"grid_path\"], study_area)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading data\n",
    "\n",
    "### Create spatiotemporal query using a STAC API as the backend\n",
    "This establishes the spatial and temporal extent used to search for Landsat satellite data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the USGS STAC API to identify scenes to load\n",
    "query = {\n",
    "    \"bbox\": bbox,\n",
    "    \"datetime\": (str(start_year - 1), str(end_year + 1)),\n",
    "}\n",
    "\n",
    "stac_query = {\n",
    "    \"query\": {\n",
    "        \"landsat:collection_category\": {\"in\": [\"T1\"]},\n",
    "        \"eo:cloud_cover\": {\"lte\": \"95.0\"}\n",
    "    }\n",
    "}\n",
    "stac_query.update(query)\n",
    "\n",
    "items = opinionated_stac_search(config, stac_query)\n",
    "\n",
    "print(f\"Found {len(items)} items matching the search criteria\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data using dask\n",
    "ds = load_and_mask_data_with_stac(items, query, log)\n",
    "ds"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tidal modelling\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Interpolate tides into each satellite timestep\n",
    "For each satellite timestep, model tide heights into a low-resolution 5 x 5 km grid (matching resolution of the FES2014 tidal model), then reproject modelled tides into the spatial extent of our satellite image. Add  this new data as a new variable in our satellite dataset to allow each satellite pixel to be analysed and filtered/masked based on the tide height at the exact moment of satellite image acquisition. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds[\"tide_m\"], tides_lowres = pixel_tides(ds, resample=True, directory=\"/home/jovyan/tide_models/\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot example interpolated tide surface for a single timestep:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot\n",
    "timestep = 15\n",
    "ds_i = ds[\"tide_m\"].isel(time=timestep)\n",
    "ds_lowres_i = tides_lowres.isel(time=timestep)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 8))\n",
    "ds_lowres_i.plot.imshow(\n",
    "    ax=axes[0],\n",
    "    robust=True,\n",
    "    cmap=\"viridis\",\n",
    "    vmin=ds_i.min().item(),\n",
    "    vmax=ds_i.max().item(),\n",
    ")\n",
    "ds_i.plot.imshow(\n",
    "    ax=axes[1],\n",
    "    robust=True,\n",
    "    cmap=\"viridis\",\n",
    "    vmin=ds_i.min().item(),\n",
    "    vmax=ds_i.max().item(),\n",
    ")\n",
    "for ax in axes:\n",
    "    gpd.GeoDataFrame(index=[0], crs=ds.odc.crs, geometry=[geometry.to_crs(ds.odc.crs)]).plot(\n",
    "        ax=ax, facecolor=\"none\", edgecolor=\"black\"\n",
    "    )\n",
    "axes[0].set_title(\"Low resolution (5 x 5 km) modelled tides\")\n",
    "axes[1].set_title(\"Modelled tides reprojected into input satellite grid\");"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate per-pixel tide cutoffs\n",
    "Based on the entire time-series of tide heights, compute the max and min satellite-observed tide height for each pixel, then calculate tide cutoffs used to restrict our data to satellite observations centred over mid-tide (0 m Above Mean Sea Level)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine tide cutoff\n",
    "tide_cutoff_min, tide_cutoff_max = coastlines.raster.tide_cutoffs(ds, tides_lowres, tide_centre=0.0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate yearly composites in memory\n",
    "Export tidally-masked MNDWI median composites for each year, and three-yearly composites used to gapfill poor data coverage areas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a yearly dataset, loaded into memory. This takes a long time!\n",
    "combined_ds = generate_yearly_combined_ds(ds, tide_cutoff_min, tide_cutoff_max, start_year, end_year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optionally dump the loaded data to disk, so that it's possible to load the second step without\n",
    "# the huge computation.\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "\n",
    "out_data = f\"data/interim/raster/{raster_version}/{study_area.replace(',', '_')}_{raster_version}_combined_ds.zarr\"\n",
    "out_path = Path(out_data)\n",
    "\n",
    "if out_path.exists():\n",
    "    print(f\"Folder {out_path} already exists. Deleting...\")\n",
    "    shutil.rmtree(out_path)\n",
    "\n",
    "combined_ds.to_zarr(out_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_ds = xr.open_zarr(f\"data/interim/raster/{raster_version}/{study_area.replace(',', '_')}_{raster_version}_combined_ds.zarr\").load()\n",
    "combined_ds"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load vector data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Coastal mask modifications\n",
    "modifications_gdf = gpd.read_file(\n",
    "    config[\"Input files\"][\"modifications_path\"], bbox=bbox\n",
    ").to_crs(str(combined_ds.odc.crs))\n",
    "\n",
    "# Mask dataset to focus on coastal zone only\n",
    "masked_ds, certainty_masks = coastlines.vector.contours_preprocess(\n",
    "    combined_ds=combined_ds,\n",
    "    water_index=water_index,\n",
    "    index_threshold=index_threshold,\n",
    "    mask_with_esa_wc=True,\n",
    "    buffer_pixels=33,\n",
    "    mask_modifications=modifications_gdf, \n",
    ")\n",
    "\n",
    "# Plot timestep\n",
    "masked_ds.isel(year=0).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract shorelines\n",
    "contours_gdf = subpixel_contours(\n",
    "    da=masked_ds,\n",
    "    z_values=index_threshold,\n",
    "    min_vertices=10,\n",
    "    dim=\"year\",\n",
    ").set_index(\"year\")\n",
    "\n",
    "# Plot shorelines\n",
    "contours_gdf.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview the data on a web map\n",
    "bb = masked_ds.odc.geobox.boundingbox.to_crs(4326)\n",
    "d_x = bb.right - bb.left\n",
    "d_y = bb.top - bb.bottom\n",
    "\n",
    "location = (bb.bottom + d_y / 2, bb.left + d_x / 2) \n",
    "\n",
    "map = folium.Map(\n",
    "  location=location,\n",
    "  tiles='https://server.arcgisonline.com/ArcGIS/rest/services/World_Imagery/MapServer/tile/{z}/{y}/{x}',\n",
    "  attr='Esri',\n",
    "  name='Esri Satellite',\n",
    "  zoom_start=10\n",
    ")\n",
    "\n",
    "for _, y in contours_gdf.to_crs(\"epsg:4326\").iterrows():\n",
    "    for r in y:\n",
    "      sim_geo = gpd.GeoSeries(r.geoms)\n",
    "      geo_j = sim_geo.to_json()\n",
    "      geo_j = folium.GeoJson(data=geo_j, style_function=lambda x: {\"stroke\": \"red\"})\n",
    "      geo_j.add_to(map)\n",
    "map"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute statistics\n",
    "\n",
    "###  Create stats points on baseline shorline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract statistics modelling points along baseline shoreline\n",
    "try:\n",
    "    points_gdf = coastlines.vector.points_on_line(contours_gdf, baseline_year, distance=30)\n",
    "except KeyError:\n",
    "    print(\"Failed to make points\")\n",
    "    points_gdf = None"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Measure annual coastline movements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if points_gdf is not None and len(points_gdf) > 0:\n",
    "    \n",
    "    # Calculate annual movements for every shoreline\n",
    "    # compared to the baseline year\n",
    "    points_gdf = coastlines.vector.annual_movements(\n",
    "        points_gdf,\n",
    "        contours_gdf,\n",
    "        combined_ds,\n",
    "        str(baseline_year),\n",
    "        water_index,\n",
    "        max_valid_dist=1200,\n",
    "    )\n",
    "    \n",
    "    # Reindex to add any missing annual columns to the dataset\n",
    "    points_gdf = points_gdf.reindex(\n",
    "        columns=[\n",
    "            \"geometry\",\n",
    "            *[f\"dist_{i}\" for i in range(start_year, end_year + 1)],\n",
    "            \"angle_mean\",\n",
    "            \"angle_std\",\n",
    "        ]\n",
    "    )\n",
    "else:\n",
    "    print(\"Something went wrong! Check the points.\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate regressions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if points_gdf is not None and len(points_gdf) > 0:\n",
    "\n",
    "    # Apply regression function to each row in dataset\n",
    "    points_gdf = coastlines.vector.calculate_regressions(points_gdf)\n",
    "\n",
    "    # Add count and span of valid obs, Shoreline Change Envelope (SCE),\n",
    "    # Net Shoreline Movement (NSM) and Max/Min years\n",
    "    stats_list = [\"valid_obs\", \"valid_span\", \"sce\", \"nsm\", \"max_year\", \"min_year\"]\n",
    "    points_gdf[stats_list] = points_gdf.apply(\n",
    "        lambda x: coastlines.vector.all_time_stats(x, initial_year=start_year), axis=1\n",
    "    )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export files\n",
    "\n",
    "### Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if points_gdf is not None and len(points_gdf) > 0:\n",
    "    # Clip stats to study area extent\n",
    "    points_gdf_clipped = points_gdf.clip(gridcell_gdf)\n",
    "\n",
    "    # Set output path\n",
    "    stats_path = (\n",
    "        f\"{output_dir}/ratesofchange_{study_area}_\"\n",
    "        f\"{vector_version}_{water_index}_{index_threshold:.2f}\"\n",
    "    )\n",
    "\n",
    "    # Write as parquet\n",
    "    points_gdf_clipped.to_parquet(f\"{stats_path}.parquet\")\n",
    "\n",
    "else:\n",
    "    print(\"Not exporting because there's no points.\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shorelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(contours_gdf.index) > 0:\n",
    "    # Add tide datum details (this supports future addition of extra tide datums)\n",
    "    contours_gdf[\"tide_datum\"] = \"0.0 m AMSL\"\n",
    "\n",
    "    # Set output path\n",
    "    contour_path = (\n",
    "        f\"{output_dir}/annualshorelines_{study_area}_{vector_version}_\"\n",
    "        f\"{water_index}_{index_threshold:.2f}\"\n",
    "    )\n",
    "\n",
    "    # Clip annual shoreline contours to study area extent\n",
    "    contours_gdf_clipped = contours_gdf.clip(gridcell_gdf)\n",
    "\n",
    "    # Export to parquet\n",
    "    contours_gdf_clipped.to_parquet(f\"{contour_path}.parquet\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Close Dask client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dask_client.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "***\n",
    "\n",
    "## Additional information"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**License:** The code in this notebook is licensed under the [Apache License, Version 2.0](https://www.apache.org/licenses/LICENSE-2.0). \n",
    "Digital Earth Australia data is licensed under the [Creative Commons by Attribution 4.0](https://creativecommons.org/licenses/by/4.0/) license.\n",
    "\n",
    "**Contact:** For assistance with any of the Python code or Jupyter Notebooks in this repository, please post a [Github issue](https://github.com/GeoscienceAustralia/dea-coastlines/issues/new).\n",
    "\n",
    "**Last modified:** November 2022\n",
    "\n",
    "**To cite:**\n",
    "\n",
    "> Bishop-Taylor, R., Nanson, R., Sagar, S., Lymburner, L. (2021). Mapping Australia's dynamic coastline at mean sea level using three decades of Landsat imagery. Remote Sensing of Environment, 267, 112734. Available: https://doi.org/10.1016/j.rse.2021.112734\n",
    ">\n",
    "> Nanson, R., Bishop-Taylor, R., Sagar, S., Lymburner, L., (2022). Geomorphic insights into Australia's coastal change using a national dataset derived from the multi-decadal Landsat archive. Estuarine, Coastal and Shelf Science, 265, p.107712. Available: https://doi.org/10.1016/j.ecss.2021.107712\n",
    ">\n",
    "> Bishop-Taylor, R., Sagar, S., Lymburner, L., Alam, I., Sixsmith, J. (2019). Sub-pixel waterline extraction: characterising accuracy and sensitivity to indices and spectra. Remote Sensing, 11 (24):2984. Available: https://doi.org/10.3390/rs11242984"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
